{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a366a5a2",
   "metadata": {},
   "source": [
    "# Simple RAG Application using Langchain\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. **Simple LLM App**\n",
    "    * Load LLM\n",
    "    * Create Prompt Teamplate\n",
    "    * Merge Prompt Template & LLM to create Chain\n",
    "2. **Build RAG App**\n",
    "    * Load External Data\n",
    "    * Generate Embeddings\n",
    "    * Build Index in Vector Store (using Embeddings)\n",
    "    * Create Retrieval Chain\n",
    "    \n",
    "    \n",
    "### Installation\n",
    "\n",
    "* **pip install langchain**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f26dd51",
   "metadata": {},
   "source": [
    "## 1. Simple LLM App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e55160",
   "metadata": {},
   "source": [
    "### 1.1 Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ac1f1c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama()"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama2\") # base_url = 'http://localhost:11434'\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd4153a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not familiar with a person or product called \"Claude 3.\" Could you please provide more context or information about who or what Claude 3 is? That way, I can better understand your question and give you an accurate answer.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"Do you know about Claude 3?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77872fa4",
   "metadata": {},
   "source": [
    "### 1.2 Create Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ead6ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an Artificial Intelligence News Reporter.\"),\n",
    "    (\"user\", \"{query}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c7c84",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1.3 Merge Prompt Template & LLM to Create LangChain Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "204b6bdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['query'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are an Artificial Intelligence News Reporter.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], template='{query}'))])\n",
       "| Ollama()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_app = prompt | llm\n",
    "\n",
    "llm_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98f997ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ah, a question about the latest advancements in artificial intelligence! *excited tone* Indeed, I am aware of Claude 3, the latest and most advanced AI language model developed by Meta AI. It has been making waves in the AI community with its groundbreaking capabilities and unparalleled performance.\n",
      "\n",
      "Claude 3 is a transformer-based language model that builds upon the successes of its predecessors, Claude 1 and Claude 2. It uses a novel architecture that combines the strengths of both models to create an AI that can generate human-like text with unprecedented accuracy and fluency. The model is trained on a vast amount of text data, including books, articles, and websites, allowing it to learn the intricacies of language and produce content that is not only grammatically correct but also contextually appropriate.\n",
      "\n",
      "One of the most significant improvements of Claude 3 is its ability to generate text that is more diverse and creative. Unlike earlier models, which often produced repetitive and formulaic content, Claude 3 can craft unique and original text that is tailored to specific topics or styles. This has made it a favorite among AI researchers and developers who are looking to push the boundaries of what is possible with language generation technology.\n",
      "\n",
      "But wait, there's more! Claude 3 also boasts improved dialogue generation capabilities. It can now engage in more complex and nuanced conversations, using context and understanding to respond to questions and statements in a natural and human-like manner. This has significant implications for applications such as chatbots, virtual assistants, and language translation software.\n",
      "\n",
      "Of course, with great power comes great responsibility. As with any advanced AI technology, there are concerns about the potential misuse of Claude 3. Some have raised ethical flags about the model's ability to generate convincing but fake news articles or propaganda. Others worry about its impact on employment, as it could potentially displace human writers and journalists.\n",
      "\n",
      "However, proponents of Claude 3 argue that it can be a powerful tool for improving communication and collaboration across languages and cultures. By facilitating the sharing of ideas and knowledge, Claude 3 has the potential to bridge gaps between communities and foster greater understanding and cooperation.\n",
      "\n",
      "In conclusion, Claude 3 represents a significant breakthrough in the field of AI language generation. Its advanced capabilities have the potential to revolutionize various industries and applications, but it is crucial that we address the ethical implications of this technology to ensure its responsible use.\n"
     ]
    }
   ],
   "source": [
    "response = llm_app.invoke({\"query\": \"Do you know about Claude 3?\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2298a1b9",
   "metadata": {},
   "source": [
    "## 2. Build RAG App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17678c3a",
   "metadata": {},
   "source": [
    "### 2.1 Load External Data\n",
    "\n",
    "* **pip install beautifulsoup4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0effaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://www.anthropic.com/news/claude-3-family\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8b83771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = [\n",
    "        \"https://www.anthropic.com/news/releasing-claude-instant-1-2\",\n",
    "        \"https://www.anthropic.com/news/claude-pro\",\n",
    "        \"https://www.anthropic.com/news/claude-2\",\n",
    "        \"https://www.anthropic.com/news/claude-2-1\",\n",
    "        \"https://www.anthropic.com/news/claude-2-1-prompting\",\n",
    "        \"https://www.anthropic.com/news/claude-3-family\",\n",
    "        \"https://www.anthropic.com/claude\"\n",
    "       ] \n",
    "\n",
    "docs = []\n",
    "for url in urls:\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs.extend(loader.load())\n",
    "    \n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dde55819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Releasing Claude Instant 1.2 \\\\ AnthropicClaudeAPIResearchCompanyNewsCareersAnnouncementsReleasing Claude Instant 1.2Aug 9, 2023●1 min readBusinesses working with Claude can now access our latest version of Claude Instant, version 1.2, available through our API.\\xa0Claude Instant is our faster, lower-priced yet still very capable model, which can handle a range of tasks including casual dialogue, text analysis, summarization, and document comprehension.Claude Instant 1.2 incorporates the strengths of our latest\\xa0model Claude 2\\xa0in real-world use cases and shows significant gains in key areas like math, coding, reasoning, and safety. It generates longer, more structured responses and follows formatting instructions better. Instant 1.2 also shows improvements in quote extraction, multilingual capabilities, and question answering.Claude Instant 1.2 outperforms Claude Instant 1.1 on math and coding, achieving 58.7% on the Codex evaluation compared to 52.8% in our previous model. It also scored 86.7% on the GSM8K benchmark, compared to 80.9% for Claude Instant 1.1.Performance of Claude Instant 1.1 compared to 1.2Our latest model has also improved on safety. It hallucinates less and is more resistant to jailbreaks, as shown in our automated red-teaming evaluation.Safety evaluation of Claude models. Lower is better.Developers looking to work with Claude Instant 1.2 can now call our latest model over our API (pricing can be found here). If you’re a business and you’d like to work with us, you can indicate your interest here.RelatedSee AllAnnouncementsUpdating our Usage PolicyMay 10, 2024Product\\xa0\\xa0·\\xa0\\xa0AnnouncementsIntroducing the Claude Team plan and iOS appMay 1, 2024Interpretability\\xa0\\xa0·\\xa0\\xa0ResearchCircuits Updates – April 2024Apr 26, 2024ClaudeAPI ResearchCompanyCustomersNewsCareersPress InquiriesSupportStatusTwitterLinkedInAvailabilityTerms of Service – ConsumerTerms of Service – CommercialPrivacy PolicyUsage PolicyResponsible Disclosure PolicyCompliancePrivacy Choices© 2024 Anthropic PBC', metadata={'source': 'https://www.anthropic.com/news/releasing-claude-instant-1-2', 'title': 'Releasing Claude Instant 1.2 \\\\ Anthropic', 'description': \"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\", 'language': 'en'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca67a5a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.2 Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dddf963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings_llm = OllamaEmbeddings(model=\"llama2\") # base_url = 'http://localhost:11434'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07f151bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.4199983775615692,\n",
       " -0.34106385707855225,\n",
       " 2.3779549598693848,\n",
       " 0.24847780168056488,\n",
       " -0.13013841211795807]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embeddings_llm.embed_query(\"How are you?\")\n",
    "\n",
    "embeddings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af4cecf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 4096)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embeddings), len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0467b560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, list, 4096)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embeddings_llm.embed_documents([\n",
    "                                \"Claude 3 is latest Conversational AI Model from Anthropic.\",\n",
    "                                \"Gemini is latest Conversational AI Model from Google.\",\n",
    "                                \"Llama-2 is latest Conversational AI Model from Meta.\",\n",
    "                                \"Mixtral is latest Conversational AI Model from Mistral AI.\",\n",
    "                                \"GPT-4 is latest Conversational AI Model from OpenAI.\"\n",
    "                               ])\n",
    "\n",
    "len(embeddings), type(embeddings), len(embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3e7e62",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.3 Build Index in Vector Store\n",
    "\n",
    "* **pip install faiss-cpu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82b21e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a29c7008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x7f800c06de80>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vector_index = FAISS.from_documents(documents, embeddings_llm)\n",
    "\n",
    "vector_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268f2e20",
   "metadata": {},
   "source": [
    "There are 3 functions to create index.\n",
    "\n",
    "* **from_documents()**\n",
    "* **from_embeddings()**\n",
    "* **from_texts()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cae22539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vector_index.as_retriever()\n",
    "\n",
    "relevant_docs = retriever.invoke({\"input\": \"Do you know about Claude 3?\"})\n",
    "\n",
    "len(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "abbfd026",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Releasing Claude Instant 1.2 \\\\ AnthropicClaudeAPIResearchCompanyNewsCareersAnnouncementsReleasing Claude Instant 1.2Aug 9, 2023●1 min readBusinesses working with Claude can now access our latest version of Claude Instant, version 1.2, available through our API.\\xa0Claude Instant is our faster, lower-priced yet still very capable model, which can handle a range of tasks including casual dialogue, text analysis, summarization, and document comprehension.Claude Instant 1.2 incorporates the strengths of our latest\\xa0model Claude 2\\xa0in real-world use cases and shows significant gains in key areas like math, coding, reasoning, and safety. It generates longer, more structured responses and follows formatting instructions better. Instant 1.2 also shows improvements in quote extraction, multilingual capabilities, and question answering.Claude Instant 1.2 outperforms Claude Instant 1.1 on math and coding, achieving 58.7% on the Codex evaluation compared to 52.8% in our previous model. It also scored 86.7% on the GSM8K benchmark, compared to 80.9% for Claude Instant 1.1.Performance of Claude Instant 1.1 compared to 1.2Our latest model has also improved on safety. It hallucinates less and is more resistant to jailbreaks, as shown in our automated red-teaming evaluation.Safety evaluation of Claude models. Lower is better.Developers looking to work with Claude Instant 1.2 can now call our latest model over our API (pricing can be found here). If you’re a business and you’d like to work with us, you can indicate your interest here.RelatedSee AllAnnouncementsUpdating our Usage PolicyMay 10, 2024Product\\xa0\\xa0·\\xa0\\xa0AnnouncementsIntroducing the Claude Team plan and iOS appMay 1, 2024Interpretability\\xa0\\xa0·\\xa0\\xa0ResearchCircuits Updates – April 2024Apr 26, 2024ClaudeAPI ResearchCompanyCustomersNewsCareersPress InquiriesSupportStatusTwitterLinkedInAvailabilityTerms of Service – ConsumerTerms of Service – CommercialPrivacy PolicyUsage PolicyResponsible Disclosure PolicyCompliancePrivacy Choices© 2024 Anthropic PBC', metadata={'source': 'https://www.anthropic.com/news/releasing-claude-instant-1-2', 'title': 'Releasing Claude Instant 1.2 \\\\ Anthropic', 'description': \"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\", 'language': 'en'}),\n",
       " Document(page_content=\"analysis of charts & graphs, financials and market trends, forecastingDifferentiatorHigher intelligence than any other model available.*1M tokens available for specific use cases, please inquire. Claude 3 Sonnet strikes the ideal balance between intelligence and speed—particularly for enterprise workloads. It delivers strong performance at a lower cost compared to its peers, and is engineered for high endurance in large-scale AI deployments.Cost [Input $/million tokens | Output $/million tokens]$3 | $15Context window200KPotential usesData processing: RAG or search & retrieval over vast amounts of knowledgeSales: product recommendations, forecasting, targeted marketingTime-saving tasks: code generation, quality control, parse text from imagesDifferentiatorMore affordable than other models with similar intelligence; better for scale.Claude 3 Haiku is our fastest, most compact model for near-instant responsiveness. It answers simple queries and requests with unmatched speed. Users will be able to build seamless AI experiences that mimic human interactions.Cost [Input $/million tokens | Output $/million tokens]$0.25 | $1.25Context window200KPotential usesCustomer interactions: quick and accurate support in live interactions, translationsContent moderation: catch risky behavior or customer requestsCost-saving tasks: optimized logistics, inventory management, extract knowledge from unstructured dataDifferentiatorSmarter, faster, and more affordable than other models in its intelligence category.Model availabilityOpus and Sonnet are available to use today in our API, which is now generally available, enabling developers to sign up and start using these models immediately. Haiku will be available soon. Sonnet is powering the free experience on claude.ai, with Opus available for Claude Pro subscribers.Sonnet is also available today through Amazon Bedrock and in private preview on Google Cloud’s Vertex AI Model Garden—with Opus and Haiku coming soon to both.Smarter, faster, saferWe do not believe that model intelligence is anywhere near its limits, and we plan to release frequent updates to the Claude 3 model family over the next few months. We're also excited to release a series of features to enhance our models' capabilities, particularly for enterprise use cases and large-scale deployments. These new features will include Tool Use (aka function calling), interactive coding (aka REPL), and more advanced agentic capabilities.As we push the boundaries of AI capabilities, we’re equally committed to ensuring that our safety guardrails keep apace with these leaps in performance. Our hypothesis is that being at the frontier of AI development is the most effective way to steer its trajectory towards positive societal outcomes.We’re excited to see what you create with Claude 3 and hope you will give us feedback to make Claude an even more useful assistant and creative companion. To start building with Claude, visit anthropic.com/claude. FootnotesThis table shows comparisons to models currently available commercially that have released evals. Our model card shows comparisons to models that have been announced but not yet released, such as Gemini 1.5 Pro. In addition, we’d like to note that engineers have worked to optimize prompts and few-shot samples for evaluations and reported higher scores for a newer GPT-4T model. Source.ClaudeAPI ResearchCompanyCustomersNewsCareersPress InquiriesSupportStatusTwitterLinkedInAvailabilityTerms of Service – ConsumerTerms of Service – CommercialPrivacy PolicyUsage PolicyResponsible Disclosure PolicyCompliancePrivacy Choices© 2024 Anthropic PBC\", metadata={'source': 'https://www.anthropic.com/news/claude-3-family', 'title': 'Introducing the next generation of Claude \\\\ Anthropic', 'description': \"Today, we're announcing the Claude 3 model family, which sets new industry benchmarks across a wide range of cognitive tasks. The family includes three state-of-the-art models in ascending order of capability: Claude 3 Haiku, Claude 3 Sonnet, and Claude 3 Opus.\", 'language': 'en'}),\n",
       " Document(page_content='gets this correct regardless of where the line with the answer sits in the context, with no modification to the prompt format used in the original experiment. As a result, we believe Claude 2.1 is much more reluctant to answer when a sentence seems out of place in a longer context, and is more likely to claim it cannot answer based on the context given. This particular cause of increased reluctance wasn’t captured by evaluations targeted at real-world long context retrieval tasks.Prompting to effectively use the 200K token context windowWhat can users do if Claude is reluctant to respond to a long context retrieval question?\\xa0We’ve found that a minor prompt update produces very different outcomes in cases where Claude is capable of giving an answer, but is hesitant to do so. When running the same evaluation internally, adding just one sentence to the prompt resulted in near complete fidelity throughout Claude 2.1’s 200K context window.We achieved significantly better results on the same evaluation by adding the sentence “Here is the most relevant sentence in the context:” to the start of Claude’s response. This was enough to raise Claude 2.1’s score from 27% to 98% on the original evaluation.Essentially, by directing the model to look for relevant sentences first, the prompt overrides Claude’s reluctance to answer based on a single sentence, especially one that appears out of place in a longer document.This approach also improves Claude’s performance on single sentence answers that were within context (ie. not out of place). To demonstrate this, the revised prompt achieves 90-95% accuracy when applied to the Yahoo/Viaweb example shared earlier:We’re constantly training Claude to become more calibrated on tasks like this, and we’re grateful to the community for conducting interesting experiments and identifying ways in which we can improve.FootnotesGregory Kamradt, ‘Pressure testing Claude-2.1 200K via Needle-in-a-Haystack’, November 2023RelatedSee AllAnnouncementsUpdating our Usage PolicyMay 10, 2024Product\\xa0\\xa0·\\xa0\\xa0AnnouncementsIntroducing the Claude Team plan and iOS appMay 1, 2024Interpretability\\xa0\\xa0·\\xa0\\xa0ResearchCircuits Updates – April 2024Apr 26, 2024ClaudeAPI ResearchCompanyCustomersNewsCareersPress InquiriesSupportStatusTwitterLinkedInAvailabilityTerms of Service – ConsumerTerms of Service – CommercialPrivacy PolicyUsage PolicyResponsible Disclosure PolicyCompliancePrivacy Choices© 2024 Anthropic PBC', metadata={'source': 'https://www.anthropic.com/news/claude-2-1-prompting', 'title': 'Long context prompting for Claude 2.1 \\\\ Anthropic', 'description': \"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\", 'language': 'en'}),\n",
       " Document(page_content='Introducing Claude Pro \\\\ AnthropicClaudeAPIResearchCompanyNewsCareersAnnouncementsIntroducing Claude ProSep 7, 2023●1 min readSubscribe todayToday, we’re introducing a paid plan for our Claude.ai\\xa0chat experience, currently available in the US and UK.Since launching in July, users tell us they’ve chosen Claude.ai as their day-to-day AI assistant for its longer context windows, faster outputs, complex reasoning capabilities, and more.\\xa0Many also shared that they would value more file uploads and conversations over longer periods.With Claude Pro, subscribers can now gain 5x more usage of our latest model, Claude 2, for a monthly price of $20 (US) or £18 (UK).This means you can level up your productivity across a range of tasks, including summarizing research papers, querying contracts, and iterating further on coding projects—like this recent demo of building an interactive map.Claude Pro offers:5x more usage than our free tier provides, with the ability to send many more messagesPriority access to Claude.ai during high-traffic periodsEarly access to new features that help you get the most out of ClaudeYou can learn more about these benefits, including how to maximize your usage, here.We’re grateful for your support as we strive to build helpful, honest, and harmless systems that fuel productivity and inspire creativity.RelatedSee AllAnnouncementsUpdating our Usage PolicyMay 10, 2024Product\\xa0\\xa0·\\xa0\\xa0AnnouncementsIntroducing the Claude Team plan and iOS appMay 1, 2024Interpretability\\xa0\\xa0·\\xa0\\xa0ResearchCircuits Updates – April 2024Apr 26, 2024ClaudeAPI ResearchCompanyCustomersNewsCareersPress InquiriesSupportStatusTwitterLinkedInAvailabilityTerms of Service – ConsumerTerms of Service – CommercialPrivacy PolicyUsage PolicyResponsible Disclosure PolicyCompliancePrivacy Choices© 2024 Anthropic PBC', metadata={'source': 'https://www.anthropic.com/news/claude-pro', 'title': 'Introducing Claude Pro \\\\ Anthropic', 'description': \"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\", 'language': 'en'})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "371a1143",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title : Releasing Claude Instant 1.2 \\ Anthropic, Source: https://www.anthropic.com/news/releasing-claude-instant-1-2\n",
      "Title : Introducing the next generation of Claude \\ Anthropic, Source: https://www.anthropic.com/news/claude-3-family\n",
      "Title : Long context prompting for Claude 2.1 \\ Anthropic, Source: https://www.anthropic.com/news/claude-2-1-prompting\n",
      "Title : Introducing Claude Pro \\ Anthropic, Source: https://www.anthropic.com/news/claude-pro\n"
     ]
    }
   ],
   "source": [
    "for doc in relevant_docs:\n",
    "    print(f\"Title : {doc.metadata['title']}, Source: {doc.metadata['source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d38a6a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.4 Create Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8fc931c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, I'm familiar with Claude 3. As an AI language model, I can provide information on the latest conversational AI models, including Claude 3 from Anthropic. According to my training data and knowledge, Claude 3 is a recent advancement in conversational AI technology that offers improved performance in natural language understanding and generation. It has been designed to better understand and respond to user input, enabling more engaging and human-like conversations.\n",
      "\n",
      "Claude 3 leverages the power of deep learning algorithms and large datasets to learn patterns in language use and generate coherent and contextually appropriate responses. This allows it to converse with users in a more natural and intuitive way, making it an attractive option for businesses and organizations looking to integrate conversational AI into their products and services.\n",
      "\n",
      "While I have knowledge of Claude 3, I cannot claim to be an expert on every topic or detail related to this model. If you have specific questions or require more detailed information, please feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based on the provided context and your internal knowledge.\n",
    "Give priority to context and if you are not sure then say you are not aware of topic:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "\n",
    "#document_chain  = prompt | llm \n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "response = document_chain.invoke({\n",
    "    \"input\": \"Do you know about Claude 3?\",\n",
    "    \"context\": [Document(page_content=\"Claude 3 is latest Conversational AI Model from Anthropic.\")]\n",
    "})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0625a80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7f800c06de80>), config={'run_name': 'retrieve_documents'})\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), config={'run_name': 'format_inputs'})\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], template='\\nAnswer the following question based on the provided context and your internal knowledge.\\nGive priority to context and if you are not sure then say you are not aware of topic:\\n\\n<context>\\n{context}\\n</context>\\n\\nQuestion: {input}\\n'))])\n",
       "            | Ollama()\n",
       "            | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
       "  }), config={'run_name': 'retrieval_chain'})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06676a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"Do you know about Claude 3?\"})\n",
    "\n",
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0b84eb03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input', 'context', 'answer'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
