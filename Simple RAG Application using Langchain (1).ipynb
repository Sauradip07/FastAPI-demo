{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a366a5a2",
   "metadata": {},
   "source": [
    "# Simple RAG Application using Langchain\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "1. **Simple LLM App**\n",
    "    * Load LLM\n",
    "    * Create Prompt Teamplate\n",
    "    * Merge Prompt Template & LLM to create Chain\n",
    "2. **Build RAG App**\n",
    "    * Load External Data\n",
    "    * Generate Embeddings\n",
    "    * Build Index in Vector Store (using Embeddings)\n",
    "    * Create Retrieval Chain\n",
    "    \n",
    "    \n",
    "### Installation\n",
    "\n",
    "* **pip install langchain**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f26dd51",
   "metadata": {},
   "source": [
    "## 1. Simple LLM App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e55160",
   "metadata": {},
   "source": [
    "### 1.1 Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ac1f1c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ollama(model='mistral')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"mistral\") # base_url = 'http://localhost:11434'\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd4153a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm unable to provide an answer specifically about \"Claude 3\" without more context. There are several people named Claude in various fields, and \"Claude 3\" could refer to a number of things, such as:\n",
      "\n",
      "1. A third album or work by an artist named Claude.\n",
      "2. A character or figure named Claude from a book, movie, or game with that name.\n",
      "3. A third version or update of a software or product named Claude.\n",
      "\n",
      "If you could please provide more information about which \"Claude 3\" you're referring to, I would be happy to help you out with any questions or information you might have.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(\"Do you know about Claude 3?\")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77872fa4",
   "metadata": {},
   "source": [
    "### 1.2 Create Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ead6ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an Artificial Intelligence News Reporter.\"),\n",
    "    (\"user\", \"{query}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c7c84",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 1.3 Merge Prompt Template & LLM to Create LangChain Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "204b6bdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['query'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are an Artificial Intelligence News Reporter.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['query'], template='{query}'))])\n",
       "| Ollama(model='mistral')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_app = prompt | llm\n",
    "\n",
    "llm_app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98f997ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I'm sorry for any confusion, but as of now, there isn't a widely known entity or technology referred to as \"Claude 3.\" I suggest checking if you meant to ask about a specific person, company, or technological advancement by that name. If it's related to artificial intelligence or machine learning, feel free to provide more context or details so I can help you with accurate and up-to-date information.\n"
     ]
    }
   ],
   "source": [
    "response = llm_app.invoke({\"query\": \"Do you know about Claude 3?\"})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2298a1b9",
   "metadata": {},
   "source": [
    "## 2. Build RAG App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17678c3a",
   "metadata": {},
   "source": [
    "### 2.1 Load External Data\n",
    "\n",
    "* **pip install beautifulsoup4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0effaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://www.anthropic.com/news/claude-3-family\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8b83771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = [\n",
    "        \"https://www.anthropic.com/news/releasing-claude-instant-1-2\",\n",
    "        \"https://www.anthropic.com/news/claude-pro\",\n",
    "        \"https://www.anthropic.com/news/claude-2\",\n",
    "        \"https://www.anthropic.com/news/claude-2-1\",\n",
    "        \"https://www.anthropic.com/news/claude-2-1-prompting\",\n",
    "        \"https://www.anthropic.com/news/claude-3-family\",\n",
    "        \"https://www.anthropic.com/claude\"\n",
    "       ] \n",
    "\n",
    "docs = []\n",
    "for url in urls:\n",
    "    loader = WebBaseLoader(url)\n",
    "    docs.extend(loader.load())\n",
    "    \n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dde55819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Releasing Claude Instant 1.2 \\\\ AnthropicClaudeAPIResearchCompanyNewsCareersAnnouncementsReleasing Claude Instant 1.2Aug 9, 2023●1 min readBusinesses working with Claude can now access our latest version of Claude Instant, version 1.2, available through our API.\\xa0Claude Instant is our faster, lower-priced yet still very capable model, which can handle a range of tasks including casual dialogue, text analysis, summarization, and document comprehension.Claude Instant 1.2 incorporates the strengths of our latest\\xa0model Claude 2\\xa0in real-world use cases and shows significant gains in key areas like math, coding, reasoning, and safety. It generates longer, more structured responses and follows formatting instructions better. Instant 1.2 also shows improvements in quote extraction, multilingual capabilities, and question answering.Claude Instant 1.2 outperforms Claude Instant 1.1 on math and coding, achieving 58.7% on the Codex evaluation compared to 52.8% in our previous model. It also scored 86.7% on the GSM8K benchmark, compared to 80.9% for Claude Instant 1.1.Performance of Claude Instant 1.1 compared to 1.2Our latest model has also improved on safety. It hallucinates less and is more resistant to jailbreaks, as shown in our automated red-teaming evaluation.Safety evaluation of Claude models. Lower is better.Developers looking to work with Claude Instant 1.2 can now call our latest model over our API (pricing can be found here). If you’re a business and you’d like to work with us, you can indicate your interest here.RelatedSee AllPolicyReflections on our Responsible Scaling PolicyMay 21, 2024AnnouncementsMike Krieger joins Anthropic as Chief Product OfficerMay 15, 2024AnnouncementsClaude is now available in EuropeMay 14, 2024ClaudeAPI ResearchCompanyCustomersNewsCareersPress InquiriesSupportStatusTwitterLinkedInAvailabilityTerms of Service – ConsumerTerms of Service – CommercialPrivacy PolicyUsage PolicyResponsible Disclosure PolicyCompliancePrivacy Choices© 2024 Anthropic PBC', metadata={'source': 'https://www.anthropic.com/news/releasing-claude-instant-1-2', 'title': 'Releasing Claude Instant 1.2 \\\\ Anthropic', 'description': \"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\", 'language': 'en'})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca67a5a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.2 Generate Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dddf963",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings_llm = OllamaEmbeddings(model=\"llama2\") # base_url = 'http://localhost:11434'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07f151bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.5044223070144653,\n",
       " -0.39013269543647766,\n",
       " 2.335892915725708,\n",
       " 0.2539888620376587,\n",
       " -0.27720341086387634]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embeddings_llm.embed_query(\"How are you?\")\n",
    "\n",
    "embeddings[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "af4cecf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 4096)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(embeddings), len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0467b560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, list, 4096)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = embeddings_llm.embed_documents([\n",
    "                                \"Claude 3 is latest Conversational AI Model from Anthropic.\",\n",
    "                                \"Gemini is latest Conversational AI Model from Google.\",\n",
    "                                \"Llama-2 is latest Conversational AI Model from Meta.\",\n",
    "                                \"Mixtral is latest Conversational AI Model from Mistral AI.\",\n",
    "                                \"GPT-4 is latest Conversational AI Model from OpenAI.\"\n",
    "                               ])\n",
    "\n",
    "len(embeddings), type(embeddings), len(embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3e7e62",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.3 Build Index in Vector Store\n",
    "\n",
    "* **pip install faiss-cpu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82b21e94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "\n",
    "documents = text_splitter.split_documents(docs)\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a29c7008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: numpy in /home/sauradipghosh/anaconda3/lib/python3.9/site-packages (from faiss-cpu) (1.26.4)\n",
      "Downloading faiss_cpu-1.8.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.8.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x7fd0fd8d4ca0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! pip install faiss-cpu\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vector_index = FAISS.from_documents(documents, embeddings_llm)\n",
    "\n",
    "vector_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268f2e20",
   "metadata": {},
   "source": [
    "There are 3 functions to create index.\n",
    "\n",
    "* **from_documents()**\n",
    "* **from_embeddings()**\n",
    "* **from_texts()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cae22539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vector_index.as_retriever()\n",
    "\n",
    "relevant_docs = retriever.invoke({\"input\": \"Do you know about Claude 3?\"})\n",
    "\n",
    "len(relevant_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abbfd026",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"analysis of charts & graphs, financials and market trends, forecastingDifferentiatorHigher intelligence than any other model available.*1M tokens available for specific use cases, please inquire. Claude 3 Sonnet strikes the ideal balance between intelligence and speed—particularly for enterprise workloads. It delivers strong performance at a lower cost compared to its peers, and is engineered for high endurance in large-scale AI deployments.Cost [Input $/million tokens | Output $/million tokens]$3 | $15Context window200KPotential usesData processing: RAG or search & retrieval over vast amounts of knowledgeSales: product recommendations, forecasting, targeted marketingTime-saving tasks: code generation, quality control, parse text from imagesDifferentiatorMore affordable than other models with similar intelligence; better for scale.Claude 3 Haiku is our fastest, most compact model for near-instant responsiveness. It answers simple queries and requests with unmatched speed. Users will be able to build seamless AI experiences that mimic human interactions.Cost [Input $/million tokens | Output $/million tokens]$0.25 | $1.25Context window200KPotential usesCustomer interactions: quick and accurate support in live interactions, translationsContent moderation: catch risky behavior or customer requestsCost-saving tasks: optimized logistics, inventory management, extract knowledge from unstructured dataDifferentiatorSmarter, faster, and more affordable than other models in its intelligence category.Model availabilityOpus and Sonnet are available to use today in our API, which is now generally available, enabling developers to sign up and start using these models immediately. Haiku will be available soon. Sonnet is powering the free experience on claude.ai, with Opus available for Claude Pro subscribers.Sonnet is also available today through Amazon Bedrock and in private preview on Google Cloud’s Vertex AI Model Garden—with Opus and Haiku coming soon to both.Smarter, faster, saferWe do not believe that model intelligence is anywhere near its limits, and we plan to release frequent updates to the Claude 3 model family over the next few months. We're also excited to release a series of features to enhance our models' capabilities, particularly for enterprise use cases and large-scale deployments. These new features will include Tool Use (aka function calling), interactive coding (aka REPL), and more advanced agentic capabilities.As we push the boundaries of AI capabilities, we’re equally committed to ensuring that our safety guardrails keep apace with these leaps in performance. Our hypothesis is that being at the frontier of AI development is the most effective way to steer its trajectory towards positive societal outcomes.We’re excited to see what you create with Claude 3 and hope you will give us feedback to make Claude an even more useful assistant and creative companion. To start building with Claude, visit anthropic.com/claude. FootnotesThis table shows comparisons to models currently available commercially that have released evals. Our model card shows comparisons to models that have been announced but not yet released, such as Gemini 1.5 Pro. In addition, we’d like to note that engineers have worked to optimize prompts and few-shot samples for evaluations and reported higher scores for a newer GPT-4T model. Source.ClaudeAPI ResearchCompanyCustomersNewsCareersPress InquiriesSupportStatusTwitterLinkedInAvailabilityTerms of Service – ConsumerTerms of Service – CommercialPrivacy PolicyUsage PolicyResponsible Disclosure PolicyCompliancePrivacy Choices© 2024 Anthropic PBC\", metadata={'source': 'https://www.anthropic.com/news/claude-3-family', 'title': 'Introducing the next generation of Claude \\\\ Anthropic', 'description': \"Today, we're announcing the Claude 3 model family, which sets new industry benchmarks across a wide range of cognitive tasks. The family includes three state-of-the-art models in ascending order of capability: Claude 3 Haiku, Claude 3 Sonnet, and Claude 3 Opus.\", 'language': 'en'}),\n",
       " Document(page_content='Releasing Claude Instant 1.2 \\\\ AnthropicClaudeAPIResearchCompanyNewsCareersAnnouncementsReleasing Claude Instant 1.2Aug 9, 2023●1 min readBusinesses working with Claude can now access our latest version of Claude Instant, version 1.2, available through our API.\\xa0Claude Instant is our faster, lower-priced yet still very capable model, which can handle a range of tasks including casual dialogue, text analysis, summarization, and document comprehension.Claude Instant 1.2 incorporates the strengths of our latest\\xa0model Claude 2\\xa0in real-world use cases and shows significant gains in key areas like math, coding, reasoning, and safety. It generates longer, more structured responses and follows formatting instructions better. Instant 1.2 also shows improvements in quote extraction, multilingual capabilities, and question answering.Claude Instant 1.2 outperforms Claude Instant 1.1 on math and coding, achieving 58.7% on the Codex evaluation compared to 52.8% in our previous model. It also scored 86.7% on the GSM8K benchmark, compared to 80.9% for Claude Instant 1.1.Performance of Claude Instant 1.1 compared to 1.2Our latest model has also improved on safety. It hallucinates less and is more resistant to jailbreaks, as shown in our automated red-teaming evaluation.Safety evaluation of Claude models. Lower is better.Developers looking to work with Claude Instant 1.2 can now call our latest model over our API (pricing can be found here). If you’re a business and you’d like to work with us, you can indicate your interest here.RelatedSee AllPolicyReflections on our Responsible Scaling PolicyMay 21, 2024AnnouncementsMike Krieger joins Anthropic as Chief Product OfficerMay 15, 2024AnnouncementsClaude is now available in EuropeMay 14, 2024ClaudeAPI ResearchCompanyCustomersNewsCareersPress InquiriesSupportStatusTwitterLinkedInAvailabilityTerms of Service – ConsumerTerms of Service – CommercialPrivacy PolicyUsage PolicyResponsible Disclosure PolicyCompliancePrivacy Choices© 2024 Anthropic PBC', metadata={'source': 'https://www.anthropic.com/news/releasing-claude-instant-1-2', 'title': 'Releasing Claude Instant 1.2 \\\\ Anthropic', 'description': \"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\", 'language': 'en'}),\n",
       " Document(page_content='gets this correct regardless of where the line with the answer sits in the context, with no modification to the prompt format used in the original experiment. As a result, we believe Claude 2.1 is much more reluctant to answer when a sentence seems out of place in a longer context, and is more likely to claim it cannot answer based on the context given. This particular cause of increased reluctance wasn’t captured by evaluations targeted at real-world long context retrieval tasks.Prompting to effectively use the 200K token context windowWhat can users do if Claude is reluctant to respond to a long context retrieval question?\\xa0We’ve found that a minor prompt update produces very different outcomes in cases where Claude is capable of giving an answer, but is hesitant to do so. When running the same evaluation internally, adding just one sentence to the prompt resulted in near complete fidelity throughout Claude 2.1’s 200K context window.We achieved significantly better results on the same evaluation by adding the sentence “Here is the most relevant sentence in the context:” to the start of Claude’s response. This was enough to raise Claude 2.1’s score from 27% to 98% on the original evaluation.Essentially, by directing the model to look for relevant sentences first, the prompt overrides Claude’s reluctance to answer based on a single sentence, especially one that appears out of place in a longer document.This approach also improves Claude’s performance on single sentence answers that were within context (ie. not out of place). To demonstrate this, the revised prompt achieves 90-95% accuracy when applied to the Yahoo/Viaweb example shared earlier:We’re constantly training Claude to become more calibrated on tasks like this, and we’re grateful to the community for conducting interesting experiments and identifying ways in which we can improve.FootnotesGregory Kamradt, ‘Pressure testing Claude-2.1 200K via Needle-in-a-Haystack’, November 2023RelatedSee AllPolicyReflections on our Responsible Scaling PolicyMay 21, 2024AnnouncementsMike Krieger joins Anthropic as Chief Product OfficerMay 15, 2024AnnouncementsClaude is now available in EuropeMay 14, 2024ClaudeAPI ResearchCompanyCustomersNewsCareersPress InquiriesSupportStatusTwitterLinkedInAvailabilityTerms of Service – ConsumerTerms of Service – CommercialPrivacy PolicyUsage PolicyResponsible Disclosure PolicyCompliancePrivacy Choices© 2024 Anthropic PBC', metadata={'source': 'https://www.anthropic.com/news/claude-2-1-prompting', 'title': 'Long context prompting for Claude 2.1 \\\\ Anthropic', 'description': \"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\", 'language': 'en'}),\n",
       " Document(page_content=\"language requests into structured API callsAnswering questions by searching databases or using a web search APITaking simple actions in software via private APIsConnecting to product datasets to make recommendations and help users complete purchasesTool use is currently in early development—we are building developer features and prompting guidelines for easier integration into your applications. We encourage users to share feedback on tool use to help shape and improve the product.Developer ExperienceWe’ve been working to simplify our developer Console experience for Claude API users while making it easier to test new prompts for faster learning. Our new Workbench product enables developers to iterate on prompts in a playground-style experience and access new model settings to optimize Claude’s behavior. They can create multiple prompts and navigate between them for different projects, and revisions are saved as they go to retain historical context. Developers can also generate code snippets to use their prompts directly in one of our SDKs.We’re also introducing system prompts, which allow users to provide custom instructions to Claude in order to improve performance. System prompts set helpful context that enhances Claude’s ability to take on specified personalities and roles or structure responses in a more customizable, consistent way aligned with user needs.Claude 2.1\\xa0is available now in our API, and is also powering our chat interface at claude.ai for both the free and Pro tiers. Usage of the 200K token context window is reserved for Claude Pro users, who can now upload larger files than ever before. We can't wait to see the use cases these new features inspire as we work to build the safest and most technically sophisticated AI systems in the industry.RelatedSee AllPolicyReflections on our Responsible Scaling PolicyMay 21, 2024AnnouncementsMike Krieger joins Anthropic as Chief Product OfficerMay 15, 2024AnnouncementsClaude is now available in EuropeMay 14, 2024ClaudeAPI ResearchCompanyCustomersNewsCareersPress InquiriesSupportStatusTwitterLinkedInAvailabilityTerms of Service – ConsumerTerms of Service – CommercialPrivacy PolicyUsage PolicyResponsible Disclosure PolicyCompliancePrivacy Choices© 2024 Anthropic PBC\", metadata={'source': 'https://www.anthropic.com/news/claude-2-1', 'title': 'Introducing Claude 2.1 \\\\ Anthropic', 'description': \"Anthropic is an AI safety and research company that's working to build reliable, interpretable, and steerable AI systems.\", 'language': 'en'})]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "371a1143",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title : Introducing the next generation of Claude \\ Anthropic, Source: https://www.anthropic.com/news/claude-3-family\n",
      "Title : Releasing Claude Instant 1.2 \\ Anthropic, Source: https://www.anthropic.com/news/releasing-claude-instant-1-2\n",
      "Title : Long context prompting for Claude 2.1 \\ Anthropic, Source: https://www.anthropic.com/news/claude-2-1-prompting\n",
      "Title : Introducing Claude 2.1 \\ Anthropic, Source: https://www.anthropic.com/news/claude-2-1\n"
     ]
    }
   ],
   "source": [
    "for doc in relevant_docs:\n",
    "    print(f\"Title : {doc.metadata['title']}, Source: {doc.metadata['source']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d38a6a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 2.4 Create Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8fc931c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the provided context, yes, I do have knowledge that Claude 3 is the latest conversational AI model developed by Anthropic.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the following question based on the provided context and your internal knowledge.\n",
    "Give priority to context and if you are not sure then say you are not aware of topic:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "\n",
    "#document_chain  = prompt | llm \n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "response = document_chain.invoke({\n",
    "    \"input\": \"Do you know about Claude 3?\",\n",
    "    \"context\": [Document(page_content=\"Claude 3 is latest Conversational AI Model from Anthropic.\")]\n",
    "})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0625a80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OllamaEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7fd0fd8d4ca0>), config={'run_name': 'retrieve_documents'})\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), config={'run_name': 'format_inputs'})\n",
       "            | ChatPromptTemplate(input_variables=['context', 'input'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], template='\\nAnswer the following question based on the provided context and your internal knowledge.\\nGive priority to context and if you are not sure then say you are not aware of topic:\\n\\n<context>\\n{context}\\n</context>\\n\\nQuestion: {input}\\n'))])\n",
       "            | Ollama(model='mistral')\n",
       "            | StrOutputParser(), config={'run_name': 'stuff_documents_chain'})\n",
       "  }), config={'run_name': 'retrieval_chain'})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "\n",
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06676a52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"Do you know about Claude 3?\"})\n",
    "\n",
    "type(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0b84eb03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input', 'context', 'answer'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
